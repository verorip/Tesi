\chapter{Applicazioni per il MM}\label{capitolo4}
In questo capitolo vengono spiegati i requisiti per i moduli sviluppati durante lo stage e la loro implementazione.
In particolare verranno esposte le dipendenze per il modulo per i comandi vocali (sezione \ref{cap:sox}), le metodologie
per accedere ai sevizi di Google (sezione \ref{cap:google}), come avviene la comunicazione tra l'API e il programma (sezione \ref{cap:api})
e l'implementazione del modulo con Touch Board (sezione \ref{cap:touch}).

\section{Modulo per comandi vocali}\label{cap:voce}
La prima applicazione implementata, come \`e stato accennato nell'introduzione del capitolo \ref{capitolo4}, \`e
stata il controllo del MagicMirror tramite comandi vocali.
Nello specifico l'applicazione deve, tramite delle specifiche frasi,
gestire le altre applicazioni presenti nel MagicMirror, sfruttando funzioni offerte dallo stesso.\\
Nella figura \ref{fig:modulovocale} \`e rappresentata la struttura e il funzionamento del modulo.
Il microfono cattura l'audio, che viene elaborato dal software Sound eXchange (\textit{SoX}), un software per l'elaborazione e la manipolazione
dell'audio, all'interno del NodeHelper e successivamente
passato all'API Speech To Text, la quale lo inoltra al Servizio Google Speech e si mette in attesa di una risposta.
Al ritorno di questa viene passata al modulo che ha il compito di validare il comando e inoltrartlo ai moduli se
corretto.
Per poter utilizzare l'API \`e necessario installare il software SoX e fornire un'autenticazione per i servizi Google.

\begin{figure}[H]
    \includegraphics[width=1\textwidth, height=0.4\textheight]{modulovocale}
    \caption{Struttura del modulo per comandi vocali}
    \label{fig:modulovocale}
\end{figure}

\subsection{Comunicazione con l'API}\label{cap:api}
Il NodeHelper dell'applicazione si occupa di gestire lo streaming con L'API e
di mandare i risultati (o gli errori) all'applicazione tramite le funzioni messe
a disposizione dal MagicMirror esposte nella sezione \ref{cap:MMmess}.
Il seguente codice viene usato per creare un canale streaming con l'API:
\begin{lstlisting}[language=Javascript, caption={Codice per l'inoltro dell'audio al Servizio Google}, captionpos=b]
      const recognizeStream = speech.streamingRecognize(request)
        .on('error', sendSocketNotification("error"))
        .on('data', (data) =>
          if(Transcription: ${data.results[0].alternatives[0].transcript})
            sendSocketNotification('limit_reached')
          else
            sendSocketNotification('response', data.results[0])
\end{lstlisting}
\emph{speech.streamingRecognize(request)}, richiama la funzione dell'API per aprire una connessione, dove request \`e
il flusso audio.
La funzione si mette successivamente in attesa di una risposta dall'API la quale pu\`o essere di due tipi:
\begin{itemize}
\item errore, nel caso ci sia stato un errore di connessione. In questo caso viene mandato al modulo un messaggio di errore
\item dati di risposta, nel caso di risposta senza errori, ma che pu\`o essere divisa uteriormente in altre 2 risposte. La prima \`e nel caso in cui
viene raggiunto il limite di parole tradotte (Google mette a disposizione un limite giornaliero per chi vuole usufruirne gratuitamente), in questo caso
il modulo lo notificherebbe a video con un messaggio di errore. La seconda la stringa contenente la frase tradotta, cui il modulo ne validerebbe il comando,
e, in caso di risposta positiva, la inoltrerebbe ai moduli.\\[1\baselineskip]
\end{itemize}
Per passare il flusso audio alla funzione appena descritta bisogna creare una \emph{pipe}, ovvero uno strumento
per permettere a due processi di comunicare.
Il seguente codice:
\begin{lstlisting}[language=Javascript]
      // Start recording and send the microphone input to the Speech API
      record
        .start({
          sampleRateHertz: 1600,
          threshold: 0,
          verbose: false,
          recordProgram: 'sox',
          silence: '20.0'
        })
        .on('error', sendSocketNotification('error'))
        .pipe(recognizeStream);
\end{lstlisting}
la funzione \emph{record} per la registrazione dell'audio
imposta tramite il metodo \emph{.start} i settaggi dello streaming (per esempio la frequenza) e ne inizia la cattura.
La funzione \emph{.on('error')} serve per sollevare un'eccezione in caso di errore, che poi viene inoltrata al modulo.
La funzione \emph{.pipe(recognizeStream)} crea una pipe tra la funzione di registrazione e \emph{recognizeStream} descritta nel codice precedente,
passando il flusso audio direttamente alla funzione.

\subsection{Difficolt√† incontrate}
\subsubsection{Sound eXchange (SoX)}\label{cap:sox}
Per permettere all'audio di
Raspbian \`e necessario installare Sound eXchange (SoX), un software per la manipolazione
dell'audio, ottimizzato per effettuarne lo streaming.
Affinch\`e il microfono si colleghi correttamente al programma \`e necessario impostare correttamente
i valori delle varibili d'ambiente \textit{AUDIODEV} e \textit{AUDIODRIVER}.
La prima variabile corrisponde al dispositivo audio al quale il programma deve fare riferimento,
la seconda varibile al driver audio da utilizzare, di solito il predefinito \`e Advanced Linux Sound Architecture(ALSA).

\subsubsection{Autenticazione Google API}\label{cap:google}
Per poter usufruire dell'API di Google \`e necessario fornire un'autenticazione a livello
di sistema.
Per poterlo fare \`e necessario ottenere delle credenziali di sicurezza per un account Google,
attivabili tramite Google Cloud Platform Console.
Le credenziali consistono in un username, l'email dell'account Google e una chiave di sicurezza unica,
 contenuti in un file JSON che pu\`o essere scaricato e salvato in locale.
Per poter permettere al sistema di utilizzare l'API bisogna fare in modo che il file JSON con le credenziali sia
raggiungibile all'interno del sistema e per farlo bisogna creare una variabile d'ambiente con assegnato il percorso
dove si trova il file.

\section{Modulo con Touch Board}\label{cap:touch}
Nell'implementazione del modulo con la Touch Board \`e necessario aver installato nel sistema
il linguaggio di programmazione Python, descritto nel capitolo \ref{cap:python}.\\
La Touch Board si presenta come una scheda con 40 porte I/O, le quali devono essere collegate alle
GPIO della scheda RaspberryPi e con un sensore elettrico di prossimit\`a, come mostrato in figura \ref{fig:TouchBoard}, che permette di catturare i movimenti fino a 5cm di distanza.
Le librerie Python in dotazione con la scheda offrono funzioni per catturare i diversi input trasmessi come
la direzione di spostamento del dito, oppure la cattura di un tocco sulla scheda.\\
Per poter eseguire un programma Python sul MM \`e necessario usare la libreria Javascript \emph{Python-Shell}, la quale
permette di avviare una shell di Python in background e avviare, di conseguenza, i programmi.
La comunicazione tra programma Python e Node-Helper avviene tramite messaggi in JSON. Quando la scheda riceve un input,
il programma Python esegue una stampa (\emph{print()}) di un JSON contente il risultato; questa stampa viene catturata dal
Node-Helper tramite la funzione \emph{pythonshell.on('message', function (message) \{ ... \})} la quale, tramite la funzione di callback,
gestisce il corpo del messaggio.
\begin{figure}[H]
    \includegraphics[width=1\textwidth, height=0.4\textheight]{touchstruct}
    \caption{Struttura del modulo con la Touch Board}
    \label{fig:structtouch}
\end{figure}

\begin{figure}[H]
    \includegraphics[width=1\textwidth, height=0.6\textheight]{skywriter}
    \caption{Touch Board Skydriver by Piromoni}
    \label{fig:TouchBoard}
\end{figure}
