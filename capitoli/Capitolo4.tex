\chapter{Applicazioni per il MM}\label{capitolo4}
In questo capitolo vengono spiegati i requisiti dei moduli sviluppati durante lo stage e la loro implementazione.
In particolare verranno esposte le dipendenze per il modulo dei comandi vocali (sezione \ref{cap:sox}), le metodologie
per accedere ai sevizi di Google (sezione \ref{cap:google}), come avviene la comunicazione tra i servizi Google e il programma (sezione \ref{cap:api})
e l'implementazione del modulo con Touch Board (sezione \ref{cap:touch}).

\section{Modulo per comandi vocali}\label{cap:voce}
La prima applicazione implementata, \`e
stata il controllo del MM tramite comandi vocali.
Nello specifico l'applicazione deve, tramite delle opportune frasi,
gestire le altre applicazioni presenti nel MM, sfruttando funzioni offerte dallo stesso.\\
Nella figura \ref{fig:modulovocale} \`e rappresentata la struttura e il funzionamento del modulo.
Il microfono cattura l'audio, che viene elaborato, all'interno del NodeHelper, dal componente "Elaborazione Audio".
Successivamente, l'audio cos\`i elaborato, viene
passato all'API Google Speech, la quale lo inoltra al Servizio Google Speech e si mette in attesa di una risposta.
Al ritorno di questa, viene passata al modulo che ha il compito di validare il comando e di inoltrarlo in broadcast a tutti i moduli, se
corretto.
La validazione del comando \`e un semplice controllo di stringa tra il comando ricevuto dal NodeHelper e una lista di stringhe in un array che
corrispondono alla lista di comandi.
Alcuni comandi di esempio sono: \emph{mostra ora}, per mostrare l'ora, \emph{che giorno \`e?}, per mostrare
il calendario, oppure \emph{mostra -nome modulo-}, per visualizzare uno specifico modulo.\\
Per poter utilizzare l'API \`e stato necessario scaricare un software per l'elaborazione dell'audio, e,
tra le diverse possibilit\`a, \`e stato scelto il software Sound eXchange (\emph{SoX}).\\
Inoltre \`e richiesta un'autenticazione per poter usufruire dei servizi Google.

\begin{figure}[H]
    \includegraphics[width=1\textwidth, height=0.4\textheight]{modulovocale}
    \caption{Struttura del modulo per comandi vocali}
    \label{fig:modulovocale}
\end{figure}

\subsection{Comunicazione con l'API di Google}\label{cap:api}
Il NodeHelper dell'applicazione si occupa di gestire lo streaming con l'API e
di mandare i risultati (o gli errori) all'applicazione tramite i metodi di \emph{SocketClient} messi
a disposizione dal MM, esposte nella sezione \ref{cap:MMmess}.
Il seguente codice viene usato per creare un canale streaming con l'API:
\begin{lstlisting}[language=Javascript, caption={Codice per l'inoltro dell'audio al Servizio Google}, captionpos=b]
      const recognizeStream = speech.streamingRecognize(request)
        .on('error', sendSocketNotification("error"))
        .on('data', (data) =>
          if(Transcription: ${data.results[0].alternatives[0].transcript})
            sendSocketNotification('limit_reached')
          else
            sendSocketNotification('response', data.results[0])
\end{lstlisting}
\emph{speech.streamingRecognize(request)}, richiama la funzione dell'API per aprire una connessione, dove \emph{request} \`e
il flusso audio.
La funzione si mette successivamente in attesa di una risposta dall'API la quale pu\`o essere di due tipi:
\begin{itemize}
\item errore, nel caso ci sia stato un errore di connessione. In questo caso viene mandato al modulo un messaggio di errore
\item dati di risposta, nel caso di risposta senza errori, ma che pu\`o essere divisa ulteriormente in altre due risposte. La prima sia ha nel caso in cui
viene raggiunto il limite di parole tradotte (Google mette a disposizione un limite giornaliero per chi vuole usufruirne gratuitamente). In questo caso
il modulo lo notificherebbe a video con un messaggio di errore. La seconda risposta contiene una stringa con la frase tradotta,
che il modulo validerebbe come comando,
e, in caso di risposta positiva, la inoltrerebbe ai moduli.\\[1\baselineskip]
\end{itemize}
Per passare il flusso audio alla funzione appena descritta bisogna creare una \emph{pipe}, ovvero un meccanismo di Comunicazione
tra due processi tramite il quale l'output del primo processo \`e inoltrato come input al secondo processo.
Nel seguente codice:
\begin{lstlisting}[language=Javascript]
      // Start recording and send the microphone input to the Speech API
      record
        .start({
          sampleRateHertz: 1600,
          threshold: 0,
          verbose: false,
          recordProgram: 'sox',
          silence: '20.0'
        })
        .on('error', sendSocketNotification('error'))
        .pipe(recognizeStream);
\end{lstlisting}
\emph{record} \`e una funzione oggetto con ascolto di eventi che
imposta tramite il metodo \emph{.start} i settaggi dello streaming (per esempio la frequenza) e ne inizia la cattura.
L'evento \emph{.on('error')} serve per sollevare un'eccezione in caso di errore, che poi viene inoltrata al modulo.
L'evento \emph{.pipe(recognizeStream)} crea una pipe tra la funzione di registrazione e \emph{recognizeStream} descritta nel codice precedente,
passando il flusso audio direttamente alla funzione.

\subsection{Difficolt\`a incontrate}
\subsubsection{Sound eXchange (SoX)}\label{cap:sox}
Come discusso nella sezione \ref{cap:voce}, per permettere all'audio di venire correttamente elaborato
per lo streaming, \`e necessario utilizzare SoX.
Affinch\`e il microfono si colleghi correttamente al programma \`e necessario impostare correttamente
i valori delle variabili d'ambiente \textit{AUDIODEV} e \textit{AUDIODRIVER}.
La prima variabile corrisponde al dispositivo audio al quale il programma deve fare riferimento,
la seconda variabile al driver audio da utilizzare; di solito il predefinito \`e Advanced Linux Sound Architecture(ALSA).

\subsubsection{Autenticazione Google API}\label{cap:google}
Per poter usufruire delle API di Google \`e necessario fornire un'autenticazione a livello
di sistema.
Per poterlo fare bisogna ottenere delle credenziali di sicurezza per un account Google,
attivabili tramite Google Cloud Platform Console.
Le credenziali consistono in un username, l'email dell'account Google e una chiave di sicurezza unica,
 il tutto contenuto in un file JSON che pu\`o essere scaricato e salvato in locale.
Per permettere al sistema di utilizzare l'API, occorre che il file JSON, con le credenziali salvate, sia
raggiungibile all'interno del sistema e per rendere possibile ci\`o, bisogna creare una variabile d'ambiente
con il nome \emph{GOOGLE\_APPLICATION\_CREDENTIALS} nel quale viene salvato il percorso dove si trova il file.

\section{Modulo con Touch Board}\label{cap:touch}
Nell'implementazione del modulo con la Touch Board \`e necessario aver installato nel sistema
il linguaggio di programmazione Python, descritto nella sezione \ref{cap:python}.\\
La Touch Board si presenta come una scheda con 40 porte I/O (le quali devono essere collegate alle
GPIO della scheda RaspberryPi) e con un sensore elettrico di prossimit\`a, come mostrato in figura \ref{fig:TouchBoard}, che permette di catturare i movimenti fino a
5 cm di distanza.
Le librerie Python, in dotazione con la scheda, offrono funzioni per catturare i diversi input trasmessi, come ad esempio
la direzione di spostamento del dito, oppure la cattura di un tocco sulla scheda.\\
La struttura del modulo, mostrata in figura \ref{fig:structtouch}, \`e composta da un'entit\`a \emph{Riconoscimento del comando} che,
al ricevimento di un input
sulla Touch Board, elabora e riconosce il segnale ricevuto (che pu\`o essere la direzione di spostamento del dito sulla scheda oppure un tocco)
e lo inoltra al modulo.
Il comando viene validato dall'entit\`a \emph{validatore di comando} tramite semplice comparazione tra la stringa ricevuta e un array di stringhe,
dove ogni elemento corrisponde
ad un possibile comando che si pu\`o impartire. In caso di risposta positiva, il comando viene inoltrato in broadcast agli altri moduli,
con i metodi discussi nella sezione \ref{cap:MMmess}.\\
Per poter eseguire un programma Python sul MM \`e necessario usare la libreria Javascript \emph{Python-Shell}, la quale
permette di avviare una shell di Python in background e avviare, di conseguenza, i programmi.
La comunicazione tra programma Python e il NodeHelper avviene tramite messaggi in JSON. Quando la scheda riceve un input,
il programma Python comunica il risultato al NodeHelper che lo inoltra al modulo.
\begin{figure}[H]
    \includegraphics[width=1\textwidth, height=0.6\textheight]{skywriter}
    \caption{Touch Board Skydriver by Piromoni}
    \label{fig:TouchBoard}
\end{figure}

\begin{figure}[H]
    \includegraphics[width=1\textwidth, height=0.4\textheight]{touchstruct}
    \caption{Struttura del modulo con la Touch Board}
    \label{fig:structtouch}
\end{figure}
